{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, pointbiserialr\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "math = pd.read_csv(\"./student-mat.csv\", sep=';', header=0)\n",
    "por = pd.read_csv(\"./student-por.csv\", sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Correlations and P-values with Target Variable:\n",
      "G2: Correlation = 0.90, P-value = 7.626e-148\n",
      "G1: Correlation = 0.80, P-value = 9.001e-90\n",
      "failures: Correlation = 0.36, P-value = 1.466e-13\n",
      "Medu: Correlation = 0.22, P-value = 1.336e-05\n",
      "higher: Correlation = 0.18, P-value = 2.668e-04\n",
      "age: Correlation = 0.16, P-value = 1.271e-03\n",
      "Fedu: Correlation = 0.15, P-value = 2.380e-03\n",
      "goout: Correlation = 0.13, P-value = 8.229e-03\n",
      "romantic: Correlation = 0.13, P-value = 9.713e-03\n",
      "reason: Correlation = 0.12, P-value = 1.527e-02\n",
      "traveltime: Correlation = 0.12, P-value = 1.987e-02\n",
      "address: Correlation = 0.11, P-value = 3.563e-02\n",
      "sex: Correlation = 0.10, P-value = 3.987e-02\n",
      "Mjob: Correlation = 0.10, P-value = 4.259e-02\n",
      "paid: Correlation = 0.10, P-value = 4.277e-02\n",
      "internet: Correlation = 0.10, P-value = 5.048e-02\n",
      "studytime: Correlation = 0.10, P-value = 5.206e-02\n",
      "schoolsup: Correlation = 0.08, P-value = 1.004e-01\n",
      "famsize: Correlation = 0.08, P-value = 1.062e-01\n",
      "guardian: Correlation = 0.07, P-value = 1.643e-01\n",
      "health: Correlation = 0.06, P-value = 2.239e-01\n",
      "Pstatus: Correlation = 0.06, P-value = 2.501e-01\n",
      "Dalc: Correlation = 0.05, P-value = 2.785e-01\n",
      "Walc: Correlation = 0.05, P-value = 3.032e-01\n",
      "nursery: Correlation = 0.05, P-value = 3.066e-01\n",
      "famrel: Correlation = 0.05, P-value = 3.086e-01\n",
      "school: Correlation = 0.05, P-value = 3.722e-01\n",
      "Fjob: Correlation = 0.04, P-value = 4.020e-01\n",
      "famsup: Correlation = 0.04, P-value = 4.377e-01\n",
      "absences: Correlation = 0.03, P-value = 4.973e-01\n",
      "activities: Correlation = 0.02, P-value = 7.497e-01\n",
      "freetime: Correlation = 0.01, P-value = 8.227e-01\n"
     ]
    }
   ],
   "source": [
    "#Feature selection based on the correlation of the dependent variables with the independent variables\n",
    "\n",
    "from scipy.stats import pearsonr, pointbiserialr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Specify the target column\n",
    "target_column = 'G3'  # Target column name\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_features = por.select_dtypes(include=['number']).columns\n",
    "categorical_features = por.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Dictionary to store correlation results\n",
    "correlation_results = {}\n",
    "\n",
    "# Handle numerical columns\n",
    "for col in numerical_features:\n",
    "    if col != target_column:\n",
    "        correlation, p_value = pearsonr(math[col], math[target_column])\n",
    "        correlation_results[col] = {'correlation': abs(correlation), 'p_value': p_value}\n",
    "\n",
    "# Handle categorical columns\n",
    "for col in categorical_features:\n",
    "    if col != target_column:\n",
    "        # Encode categorical values\n",
    "        encoded_col = LabelEncoder().fit_transform(math[col])\n",
    "        correlation, p_value = pointbiserialr(encoded_col, math[target_column])\n",
    "        correlation_results[col] = {'correlation': abs(correlation), 'p_value': p_value}\n",
    "\n",
    "# Sort features by correlation\n",
    "sorted_features = sorted(correlation_results.items(), key=lambda x: x[1]['correlation'], reverse=True)\n",
    "\n",
    "# Display top features with p-values\n",
    "print(\"Feature Correlations and P-values with Target Variable:\")\n",
    "for feature, stats in sorted_features:\n",
    "    print(f\"{feature}: Correlation = {stats['correlation']:.2f}, P-value = {stats['p_value']:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR with all the features with splitting into test set and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions vs Actual:\n",
      "[[ 7.7   8.  ]\n",
      " [14.54 15.  ]\n",
      " [16.22 16.  ]\n",
      " [10.5  10.  ]\n",
      " [ 9.45 10.  ]\n",
      " [12.58 12.  ]\n",
      " [12.98 13.  ]\n",
      " [18.31 17.  ]\n",
      " [12.11 12.  ]\n",
      " [11.41 12.  ]\n",
      " [10.5  11.  ]\n",
      " [10.64 10.  ]\n",
      " [13.2  13.  ]\n",
      " [ 8.69  8.  ]\n",
      " [18.19 18.  ]\n",
      " [12.44 12.  ]\n",
      " [12.75 13.  ]\n",
      " [12.55 13.  ]\n",
      " [10.68 10.  ]\n",
      " [10.34 10.  ]\n",
      " [12.16 12.  ]\n",
      " [10.53 10.  ]\n",
      " [17.38 17.  ]\n",
      " [13.37 15.  ]\n",
      " [12.59 14.  ]\n",
      " [ 1.53  0.  ]\n",
      " [12.71 12.  ]\n",
      " [13.52 14.  ]\n",
      " [11.36 12.  ]\n",
      " [12.69  9.  ]\n",
      " [13.61 13.  ]\n",
      " [16.28 16.  ]\n",
      " [13.25 13.  ]\n",
      " [16.15 16.  ]\n",
      " [12.41 12.  ]\n",
      " [ 9.21 10.  ]\n",
      " [ 9.52 10.  ]\n",
      " [11.38 11.  ]\n",
      " [12.54 13.  ]\n",
      " [11.41 10.  ]\n",
      " [15.46 15.  ]\n",
      " [17.37 18.  ]\n",
      " [11.5  11.  ]\n",
      " [13.66 13.  ]\n",
      " [12.54 13.  ]\n",
      " [ 9.71 10.  ]\n",
      " [12.67 14.  ]\n",
      " [ 9.2   9.  ]\n",
      " [11.45 11.  ]\n",
      " [ 9.56 10.  ]\n",
      " [ 6.21  8.  ]\n",
      " [14.71 17.  ]\n",
      " [ 9.43  9.  ]\n",
      " [12.29 13.  ]\n",
      " [ 7.28  8.  ]\n",
      " [11.12 11.  ]\n",
      " [11.65 12.  ]\n",
      " [11.02 12.  ]\n",
      " [14.61 15.  ]\n",
      " [14.64 15.  ]\n",
      " [13.4  13.  ]\n",
      " [ 7.61  7.  ]\n",
      " [11.63 12.  ]\n",
      " [ 9.3  10.  ]\n",
      " [13.48 12.  ]\n",
      " [12.33 12.  ]\n",
      " [11.95 11.  ]\n",
      " [13.29 13.  ]\n",
      " [14.4  14.  ]\n",
      " [ 6.86  8.  ]\n",
      " [ 8.46  9.  ]\n",
      " [11.35 11.  ]\n",
      " [13.75 13.  ]\n",
      " [10.85 11.  ]\n",
      " [14.25 14.  ]\n",
      " [13.54 13.  ]\n",
      " [13.8  14.  ]\n",
      " [11.61 13.  ]\n",
      " [13.65 13.  ]\n",
      " [12.33 13.  ]\n",
      " [14.5  14.  ]\n",
      " [ 9.47 11.  ]\n",
      " [10.39 10.  ]\n",
      " [13.19 14.  ]\n",
      " [18.53 17.  ]\n",
      " [11.43 13.  ]\n",
      " [10.47 10.  ]\n",
      " [13.76 12.  ]\n",
      " [13.48 13.  ]\n",
      " [13.29 10.  ]\n",
      " [11.38 12.  ]\n",
      " [15.75 16.  ]\n",
      " [18.06 17.  ]\n",
      " [11.74 11.  ]\n",
      " [ 7.7   6.  ]\n",
      " [10.25 11.  ]\n",
      " [13.53 14.  ]\n",
      " [11.37 11.  ]\n",
      " [13.6  13.  ]\n",
      " [14.41 15.  ]\n",
      " [12.37 14.  ]\n",
      " [ 9.44 10.  ]\n",
      " [ 5.84  8.  ]\n",
      " [11.08 11.  ]\n",
      " [10.35 10.  ]\n",
      " [11.28 12.  ]\n",
      " [16.43 17.  ]\n",
      " [10.76 11.  ]\n",
      " [ 9.99  9.  ]\n",
      " [15.39 15.  ]\n",
      " [12.12 12.  ]\n",
      " [13.39 13.  ]\n",
      " [14.59 14.  ]\n",
      " [13.31 14.  ]\n",
      " [11.67 12.  ]\n",
      " [10.   11.  ]\n",
      " [15.71 15.  ]\n",
      " [16.35 16.  ]\n",
      " [10.56 10.  ]\n",
      " [11.24 12.  ]\n",
      " [ 7.74  8.  ]\n",
      " [10.67 11.  ]\n",
      " [10.67 11.  ]\n",
      " [11.08 11.  ]\n",
      " [13.39 14.  ]\n",
      " [15.22 15.  ]\n",
      " [14.83 14.  ]\n",
      " [17.09 15.  ]\n",
      " [12.72 10.  ]\n",
      " [10.19 11.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8775693192610926"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the target column and features\n",
    "target_column = \"G3\"\n",
    "X = por.drop(columns=[target_column])\n",
    "y = por[target_column]\n",
    "\n",
    "# Apply the scaler only to numerical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "sc_X = StandardScaler()\n",
    "X[numerical_cols] = sc_X.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Encode categorical columns (if any)\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# Scale the target column\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y.values.reshape(-1, 1)).flatten()  # Flatten back to 1D array\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize Support Vector Regression (SVR) with RBF kernel\n",
    "regressor = SVR(kernel='linear') # linear kernel was better than rbf in this case: 0.71 vs 0.78\n",
    "\n",
    "# Fit the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "# Rescale predictions and actual values back to original scale\n",
    "y_pred_original = sc_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = sc_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "# Display predictions vs actual values\n",
    "comparison = np.column_stack((y_pred_original, y_test_original))\n",
    "print(\"Predictions vs Actual:\")\n",
    "print(comparison)\n",
    "\n",
    "# Evaluating the model performance\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2 is 0.7878634076745303 for math, \n",
    "r2 is 0.8775693192610926 for portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR with splitting the data into test and train set and with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions vs Actual:\n",
      "[[ 7.73  8.  ]\n",
      " [14.43 15.  ]\n",
      " [16.12 16.  ]\n",
      " [10.47 10.  ]\n",
      " [ 9.38 10.  ]\n",
      " [12.69 12.  ]\n",
      " [12.96 13.  ]\n",
      " [18.41 17.  ]\n",
      " [12.23 12.  ]\n",
      " [11.61 12.  ]\n",
      " [10.67 11.  ]\n",
      " [10.48 10.  ]\n",
      " [13.46 13.  ]\n",
      " [ 8.69  8.  ]\n",
      " [18.23 18.  ]\n",
      " [12.37 12.  ]\n",
      " [12.73 13.  ]\n",
      " [12.46 13.  ]\n",
      " [10.56 10.  ]\n",
      " [10.35 10.  ]\n",
      " [12.36 12.  ]\n",
      " [10.58 10.  ]\n",
      " [17.34 17.  ]\n",
      " [13.3  15.  ]\n",
      " [12.55 14.  ]\n",
      " [ 1.33  0.  ]\n",
      " [12.4  12.  ]\n",
      " [13.42 14.  ]\n",
      " [11.51 12.  ]\n",
      " [12.59  9.  ]\n",
      " [13.56 13.  ]\n",
      " [16.61 16.  ]\n",
      " [13.47 13.  ]\n",
      " [16.42 16.  ]\n",
      " [12.55 12.  ]\n",
      " [ 9.11 10.  ]\n",
      " [ 9.45 10.  ]\n",
      " [11.43 11.  ]\n",
      " [12.68 13.  ]\n",
      " [11.49 10.  ]\n",
      " [15.43 15.  ]\n",
      " [17.33 18.  ]\n",
      " [11.56 11.  ]\n",
      " [13.4  13.  ]\n",
      " [12.49 13.  ]\n",
      " [ 9.69 10.  ]\n",
      " [12.73 14.  ]\n",
      " [ 9.5   9.  ]\n",
      " [11.57 11.  ]\n",
      " [ 9.4  10.  ]\n",
      " [ 6.05  8.  ]\n",
      " [14.62 17.  ]\n",
      " [ 9.39  9.  ]\n",
      " [12.36 13.  ]\n",
      " [ 7.4   8.  ]\n",
      " [11.08 11.  ]\n",
      " [11.69 12.  ]\n",
      " [11.23 12.  ]\n",
      " [14.53 15.  ]\n",
      " [14.47 15.  ]\n",
      " [13.34 13.  ]\n",
      " [ 7.97  7.  ]\n",
      " [11.59 12.  ]\n",
      " [ 9.16 10.  ]\n",
      " [13.54 12.  ]\n",
      " [12.54 12.  ]\n",
      " [11.71 11.  ]\n",
      " [13.26 13.  ]\n",
      " [14.39 14.  ]\n",
      " [ 6.82  8.  ]\n",
      " [ 8.36  9.  ]\n",
      " [11.54 11.  ]\n",
      " [13.54 13.  ]\n",
      " [10.64 11.  ]\n",
      " [14.24 14.  ]\n",
      " [13.72 13.  ]\n",
      " [13.64 14.  ]\n",
      " [11.53 13.  ]\n",
      " [13.62 13.  ]\n",
      " [12.29 13.  ]\n",
      " [14.54 14.  ]\n",
      " [ 9.67 11.  ]\n",
      " [10.26 10.  ]\n",
      " [13.22 14.  ]\n",
      " [18.34 17.  ]\n",
      " [11.48 13.  ]\n",
      " [10.56 10.  ]\n",
      " [13.64 12.  ]\n",
      " [13.63 13.  ]\n",
      " [13.46 10.  ]\n",
      " [11.3  12.  ]\n",
      " [15.55 16.  ]\n",
      " [18.31 17.  ]\n",
      " [11.51 11.  ]\n",
      " [ 7.79  6.  ]\n",
      " [10.18 11.  ]\n",
      " [13.56 14.  ]\n",
      " [11.41 11.  ]\n",
      " [13.45 13.  ]\n",
      " [14.32 15.  ]\n",
      " [12.47 14.  ]\n",
      " [ 9.46 10.  ]\n",
      " [ 5.75  8.  ]\n",
      " [11.27 11.  ]\n",
      " [10.34 10.  ]\n",
      " [11.35 12.  ]\n",
      " [16.36 17.  ]\n",
      " [11.03 11.  ]\n",
      " [10.11  9.  ]\n",
      " [15.46 15.  ]\n",
      " [12.3  12.  ]\n",
      " [13.54 13.  ]\n",
      " [14.35 14.  ]\n",
      " [13.39 14.  ]\n",
      " [11.62 12.  ]\n",
      " [10.11 11.  ]\n",
      " [15.49 15.  ]\n",
      " [16.43 16.  ]\n",
      " [10.5  10.  ]\n",
      " [11.28 12.  ]\n",
      " [ 7.58  8.  ]\n",
      " [10.56 11.  ]\n",
      " [10.54 11.  ]\n",
      " [11.12 11.  ]\n",
      " [13.47 14.  ]\n",
      " [15.24 15.  ]\n",
      " [14.68 14.  ]\n",
      " [17.1  15.  ]\n",
      " [12.78 10.  ]\n",
      " [ 9.87 11.  ]]\n",
      "R² Score: 0.8737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[numerical_cols] = sc_X.fit_transform(X[numerical_cols])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/1903768725.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = LabelEncoder().fit_transform(X[col])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define the target column and features\n",
    "target_column = \"G3\"\n",
    "# Select only the specified columns from the dataset\n",
    "selected_columns = ['Medu', 'failures', 'Dalc', 'Walc', 'absences', 'G1', 'G2', \n",
    "                    'sex', 'Mjob', 'schoolsup', 'famsup', 'activities', 'nursery', 'higher']\n",
    "X = por[selected_columns]  # Use only the selected columns as features\n",
    "y = por[target_column]\n",
    "\n",
    "# Apply the scaler only to numerical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "sc_X = StandardScaler()\n",
    "X[numerical_cols] = sc_X.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Encode categorical columns (if any)\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# Scale the target column\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y.values.reshape(-1, 1)).flatten()  # Flatten back to 1D array\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize Support Vector Regression (SVR) with linear kernel\n",
    "regressor = SVR(kernel='linear')  # Linear kernel was better than RBF in this case\n",
    "\n",
    "# Fit the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Rescale predictions and actual values back to original scale\n",
    "y_pred_original = sc_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = sc_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "# Display predictions vs actual values\n",
    "comparison = np.column_stack((y_pred_original, y_test_original))\n",
    "print(\"Predictions vs Actual:\")\n",
    "print(comparison)\n",
    "\n",
    "# Evaluate the model performance\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions vs Actual (in fixed-point notation):\n",
      "[[ 7.73  8.  ]\n",
      " [14.43 15.  ]\n",
      " [16.12 16.  ]\n",
      " [10.47 10.  ]\n",
      " [ 9.38 10.  ]\n",
      " [12.69 12.  ]\n",
      " [12.96 13.  ]\n",
      " [18.41 17.  ]\n",
      " [12.23 12.  ]\n",
      " [11.61 12.  ]\n",
      " [10.67 11.  ]\n",
      " [10.48 10.  ]\n",
      " [13.46 13.  ]\n",
      " [ 8.69  8.  ]\n",
      " [18.23 18.  ]\n",
      " [12.37 12.  ]\n",
      " [12.73 13.  ]\n",
      " [12.46 13.  ]\n",
      " [10.56 10.  ]\n",
      " [10.35 10.  ]\n",
      " [12.36 12.  ]\n",
      " [10.58 10.  ]\n",
      " [17.34 17.  ]\n",
      " [13.3  15.  ]\n",
      " [12.55 14.  ]\n",
      " [ 1.33  0.  ]\n",
      " [12.4  12.  ]\n",
      " [13.42 14.  ]\n",
      " [11.51 12.  ]\n",
      " [12.59  9.  ]\n",
      " [13.56 13.  ]\n",
      " [16.61 16.  ]\n",
      " [13.47 13.  ]\n",
      " [16.42 16.  ]\n",
      " [12.55 12.  ]\n",
      " [ 9.11 10.  ]\n",
      " [ 9.45 10.  ]\n",
      " [11.43 11.  ]\n",
      " [12.68 13.  ]\n",
      " [11.49 10.  ]\n",
      " [15.43 15.  ]\n",
      " [17.33 18.  ]\n",
      " [11.56 11.  ]\n",
      " [13.4  13.  ]\n",
      " [12.49 13.  ]\n",
      " [ 9.69 10.  ]\n",
      " [12.73 14.  ]\n",
      " [ 9.5   9.  ]\n",
      " [11.57 11.  ]\n",
      " [ 9.4  10.  ]\n",
      " [ 6.05  8.  ]\n",
      " [14.62 17.  ]\n",
      " [ 9.39  9.  ]\n",
      " [12.36 13.  ]\n",
      " [ 7.4   8.  ]\n",
      " [11.08 11.  ]\n",
      " [11.69 12.  ]\n",
      " [11.23 12.  ]\n",
      " [14.53 15.  ]\n",
      " [14.47 15.  ]\n",
      " [13.34 13.  ]\n",
      " [ 7.97  7.  ]\n",
      " [11.59 12.  ]\n",
      " [ 9.16 10.  ]\n",
      " [13.54 12.  ]\n",
      " [12.54 12.  ]\n",
      " [11.71 11.  ]\n",
      " [13.26 13.  ]\n",
      " [14.39 14.  ]\n",
      " [ 6.82  8.  ]\n",
      " [ 8.36  9.  ]\n",
      " [11.54 11.  ]\n",
      " [13.54 13.  ]\n",
      " [10.64 11.  ]\n",
      " [14.24 14.  ]\n",
      " [13.72 13.  ]\n",
      " [13.64 14.  ]\n",
      " [11.53 13.  ]\n",
      " [13.62 13.  ]\n",
      " [12.29 13.  ]\n",
      " [14.54 14.  ]\n",
      " [ 9.67 11.  ]\n",
      " [10.26 10.  ]\n",
      " [13.22 14.  ]\n",
      " [18.34 17.  ]\n",
      " [11.48 13.  ]\n",
      " [10.56 10.  ]\n",
      " [13.64 12.  ]\n",
      " [13.63 13.  ]\n",
      " [13.46 10.  ]\n",
      " [11.3  12.  ]\n",
      " [15.55 16.  ]\n",
      " [18.31 17.  ]\n",
      " [11.51 11.  ]\n",
      " [ 7.79  6.  ]\n",
      " [10.18 11.  ]\n",
      " [13.56 14.  ]\n",
      " [11.41 11.  ]\n",
      " [13.45 13.  ]\n",
      " [14.32 15.  ]\n",
      " [12.47 14.  ]\n",
      " [ 9.46 10.  ]\n",
      " [ 5.75  8.  ]\n",
      " [11.27 11.  ]\n",
      " [10.34 10.  ]\n",
      " [11.35 12.  ]\n",
      " [16.36 17.  ]\n",
      " [11.03 11.  ]\n",
      " [10.11  9.  ]\n",
      " [15.46 15.  ]\n",
      " [12.3  12.  ]\n",
      " [13.54 13.  ]\n",
      " [14.35 14.  ]\n",
      " [13.39 14.  ]\n",
      " [11.62 12.  ]\n",
      " [10.11 11.  ]\n",
      " [15.49 15.  ]\n",
      " [16.43 16.  ]\n",
      " [10.5  10.  ]\n",
      " [11.28 12.  ]\n",
      " [ 7.58  8.  ]\n",
      " [10.56 11.  ]\n",
      " [10.54 11.  ]\n",
      " [11.12 11.  ]\n",
      " [13.47 14.  ]\n",
      " [15.24 15.  ]\n",
      " [14.68 14.  ]\n",
      " [17.1  15.  ]\n",
      " [12.78 10.  ]\n",
      " [ 9.87 11.  ]]\n",
      "R² Score: 0.8737\n",
      "Mean Squared Error: 0.8975\n",
      "Mean Absolute Error: 0.7313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 1.31 -1.34 -1.34  1.31  0.43  1.31 -0.45  1.31  0.43  0.43  1.31 -0.45\n",
      "  1.31  1.31 -0.45  1.31  1.31  0.43  0.43  1.31  1.31  1.31  1.31 -0.45\n",
      " -0.45 -0.45 -0.45  1.31  0.43  1.31  1.31  1.31  1.31  0.43  0.43 -0.45\n",
      "  1.31  1.31  0.43 -0.45 -0.45  1.31  1.31 -0.45 -0.45  1.31  0.43  1.31\n",
      "  1.31  1.31 -0.45  1.31  1.31  1.31  0.43 -0.45  1.31  1.31 -1.34  1.31\n",
      "  1.31 -1.34 -1.34  1.31  1.31  1.31  1.31  0.43 -0.45  0.43  0.43  1.31\n",
      " -1.34  0.43  0.43  1.31  1.31 -0.45 -0.45  0.43 -0.45 -0.45  0.43 -0.45\n",
      " -1.34  1.31 -0.45  1.31 -0.45  1.31  0.43  1.31  0.43  1.31 -0.45 -1.34\n",
      "  1.31 -0.45  1.31  1.31  1.31  1.31  1.31  0.43  0.43  0.43 -0.45  0.43\n",
      "  1.31  1.31  1.31  0.43 -0.45  1.31 -0.45  1.31  1.31  0.43 -1.34  0.43\n",
      " -1.34 -0.45 -0.45  1.31 -0.45  0.43  0.43 -2.22  1.31 -0.45  0.43 -0.45\n",
      " -0.45 -1.34 -0.45  0.43 -0.45 -0.45  0.43  1.31  0.43 -1.34  0.43  0.43\n",
      " -1.34  0.43 -1.34  1.31 -1.34  1.31  0.43  1.31 -0.45  1.31 -1.34 -0.45\n",
      " -1.34 -0.45  0.43 -1.34 -1.34  1.31 -0.45 -1.34 -0.45  0.43  0.43  1.31\n",
      " -0.45 -1.34 -1.34 -0.45  0.43 -1.34 -0.45  0.43 -1.34 -1.34 -1.34  0.43\n",
      " -0.45  1.31 -0.45  1.31  0.43 -1.34  1.31 -1.34  0.43  1.31 -0.45  0.43\n",
      "  1.31  1.31  1.31  0.43 -0.45  0.43  0.43  0.43 -1.34 -0.45  0.43 -1.34\n",
      " -0.45 -1.34 -1.34  0.43 -0.45 -0.45  1.31  0.43  1.31  1.31  1.31 -0.45\n",
      " -1.34 -0.45 -0.45  0.43  0.43  1.31 -1.34  1.31  0.43  1.31 -0.45 -0.45\n",
      "  1.31  0.43  1.31  0.43 -0.45 -0.45 -0.45 -1.34 -0.45 -0.45  1.31  0.43\n",
      "  0.43 -0.45 -0.45 -0.45  1.31 -0.45  1.31  1.31 -1.34  0.43 -0.45 -0.45\n",
      "  1.31 -0.45 -0.45  0.43 -0.45  1.31  0.43 -1.34 -1.34 -0.45 -1.34 -0.45\n",
      " -0.45 -1.34  1.31  1.31  1.31  1.31  1.31  1.31 -1.34  0.43 -1.34 -0.45\n",
      " -0.45 -1.34 -0.45  0.43  0.43 -2.22  0.43  0.43 -0.45 -0.45 -0.45 -1.34\n",
      " -0.45  1.31 -0.45 -0.45  1.31 -1.34 -0.45  1.31 -0.45 -0.45 -0.45 -0.45\n",
      "  1.31  1.31  0.43  0.43 -0.45 -0.45  0.43  0.43  1.31  1.31 -0.45  0.43\n",
      " -0.45 -1.34 -1.34 -0.45 -0.45 -0.45  0.43  1.31  1.31 -0.45  0.43  1.31\n",
      " -1.34  1.31  0.43 -1.34 -1.34 -0.45 -0.45 -1.34 -0.45 -1.34 -0.45  1.31\n",
      "  1.31  1.31  0.43  0.43  0.43  1.31  1.31  1.31  1.31  1.31  1.31  1.31\n",
      "  1.31  0.43 -0.45  0.43  0.43 -1.34 -0.45 -0.45  1.31  1.31  1.31  0.43\n",
      "  0.43  1.31  1.31  0.43  1.31  1.31  0.43 -0.45  1.31 -1.34 -1.34 -0.45\n",
      " -0.45 -0.45  0.43 -2.22 -1.34  1.31  0.43 -0.45  1.31  1.31  0.43 -0.45\n",
      "  0.43 -0.45 -0.45 -0.45  0.43 -0.45 -0.45  0.43  0.43  0.43  0.43  1.31\n",
      "  0.43 -0.45 -0.45  0.43  1.31  1.31  1.31 -0.45 -0.45 -0.45  0.43  1.31\n",
      " -0.45  0.43 -0.45  1.31  1.31 -1.34 -1.34  1.31  0.43  0.43 -0.45 -0.45\n",
      " -0.45 -1.34  0.43 -1.34 -0.45 -1.34  0.43 -0.45 -0.45  1.31 -1.34  0.43\n",
      " -1.34  1.31 -0.45 -1.34 -1.34 -2.22 -0.45  0.43 -1.34 -1.34  1.31  1.31\n",
      "  0.43 -1.34  0.43 -0.45  1.31 -1.34 -0.45 -1.34 -0.45 -1.34 -1.34 -0.45\n",
      " -1.34 -1.34 -1.34 -1.34 -1.34 -1.34 -1.34 -0.45 -1.34 -1.34  0.43 -0.45\n",
      " -0.45  1.31  0.43 -0.45 -0.45 -0.45 -1.34 -0.45 -0.45  0.43 -1.34 -0.45\n",
      " -0.45 -0.45 -1.34 -0.45 -0.45 -0.45 -0.45  0.43 -1.34 -1.34 -1.34 -1.34\n",
      " -0.45 -2.22 -1.34  0.43 -1.34  0.43 -0.45 -0.45 -1.34  1.31 -0.45 -1.34\n",
      " -0.45 -1.34  0.43 -1.34 -0.45 -1.34 -0.45 -1.34 -1.34  0.43  0.43 -1.34\n",
      "  1.31 -1.34 -0.45 -0.45 -1.34 -0.45  1.31  1.31  0.43  1.31  1.31 -0.45\n",
      " -0.45  1.31  0.43 -1.34 -0.45 -0.45  1.31 -1.34 -0.45 -0.45 -0.45  1.31\n",
      "  0.43 -0.45  1.31  1.31 -1.34  0.43  1.31  1.31 -1.34  1.31 -0.45  0.43\n",
      " -1.34 -1.34 -1.34 -1.34 -1.34  0.43 -0.45 -1.34 -1.34 -1.34 -0.45 -0.45\n",
      "  0.43 -1.34 -1.34  0.43  0.43  0.43 -0.45 -0.45  0.43 -1.34 -1.34  1.31\n",
      "  0.43 -1.34  0.43 -1.34 -1.34 -1.34 -1.34 -0.45 -2.22  0.43  1.31 -0.45\n",
      "  1.31 -1.34 -1.34 -0.45  0.43 -1.34  1.31  1.31  1.31 -0.45 -1.34  1.31\n",
      " -1.34 -0.45 -0.45  1.31 -1.34 -1.34  1.31 -0.45 -1.34 -1.34 -1.34 -0.45\n",
      "  1.31  0.43  1.31  0.43 -1.34 -1.34 -1.34  0.43  1.31 -1.34 -1.34  1.31\n",
      " -1.34 -0.45  1.31 -1.34 -0.45 -1.34  1.31 -1.34 -1.34  1.31  0.43  0.43\n",
      "  1.31 -0.45 -0.45 -1.34  1.31 -0.45  1.31  1.31 -0.45  0.43 -1.34  0.43\n",
      "  0.43]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  4.69 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  4.69 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  3.   -0.37 -0.37 -0.37  4.69\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37  3.   -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37  3.    1.31 -0.37 -0.37 -0.37\n",
      " -0.37  4.69  4.69 -0.37  1.31  3.    1.31  3.   -0.37  1.31 -0.37  4.69\n",
      " -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  4.69 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37  1.31  1.31  1.31  1.31 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37\n",
      "  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37  4.69 -0.37 -0.37 -0.37  1.31  3.   -0.37 -0.37  1.31\n",
      " -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37  1.31 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37  1.31  3.   -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  3.   -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31  1.31  3.\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37  3.   -0.37  1.31 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37  1.31 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37  1.31\n",
      "  1.31 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31  1.31 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  4.69 -0.37\n",
      "  1.31 -0.37 -0.37 -0.37 -0.37 -0.37  1.31  3.    1.31  1.31  1.31  1.31\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37  1.31 -0.37  1.31 -0.37\n",
      " -0.37 -0.37  1.31 -0.37  1.31 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37  4.69 -0.37  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      "  3.   -0.37 -0.37 -0.37 -0.37  4.69 -0.37  1.31 -0.37 -0.37 -0.37  1.31\n",
      " -0.37 -0.37  1.31  1.31  4.69  3.   -0.37  4.69  3.   -0.37  1.31 -0.37\n",
      " -0.37  1.31  1.31 -0.37  1.31  3.   -0.37  1.31 -0.37 -0.37 -0.37  1.31\n",
      " -0.37 -0.37  3.   -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37 -0.37  1.31  1.31 -0.37 -0.37 -0.37 -0.37  4.69 -0.37\n",
      "  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37\n",
      "  1.31 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37\n",
      " -0.37 -0.37 -0.37  1.31  1.31 -0.37 -0.37 -0.37  1.31 -0.37 -0.37 -0.37\n",
      " -0.37]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54  0.54\n",
      " -0.54 -0.54 -0.54  0.54 -0.54  3.78  1.62 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54\n",
      "  0.54 -0.54  0.54 -0.54  1.62  0.54  2.7  -0.54 -0.54 -0.54 -0.54 -0.54\n",
      "  0.54  3.78 -0.54  0.54  0.54 -0.54  3.78 -0.54 -0.54  0.54 -0.54 -0.54\n",
      "  0.54  0.54  0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      "  0.54  0.54 -0.54 -0.54 -0.54  1.62 -0.54 -0.54  0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54  3.78 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      "  1.62 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54  1.62 -0.54 -0.54  0.54  0.54  0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54  2.7   3.78\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54  0.54 -0.54  1.62 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54  0.54  0.54  0.54  0.54  1.62  0.54 -0.54 -0.54\n",
      " -0.54  1.62 -0.54 -0.54  2.7   1.62  0.54 -0.54  0.54 -0.54 -0.54 -0.54\n",
      "  0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54  2.7  -0.54 -0.54\n",
      "  1.62 -0.54  0.54 -0.54  0.54  0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54  2.7   1.62 -0.54 -0.54 -0.54  1.62  0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54  1.62  0.54 -0.54 -0.54 -0.54 -0.54  2.7  -0.54  0.54\n",
      " -0.54 -0.54  0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54  3.78 -0.54 -0.54\n",
      " -0.54 -0.54  2.7  -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54  3.78 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  1.62\n",
      " -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54  3.78 -0.54  0.54  0.54 -0.54  0.54 -0.54  0.54 -0.54\n",
      " -0.54 -0.54 -0.54  2.7  -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54  0.54  1.62  1.62  0.54 -0.54 -0.54  1.62\n",
      " -0.54 -0.54  0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54  2.7  -0.54\n",
      "  0.54  0.54  1.62 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54  0.54 -0.54  0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54\n",
      " -0.54 -0.54  1.62 -0.54 -0.54 -0.54 -0.54  0.54  1.62  2.7  -0.54  0.54\n",
      " -0.54  0.54  0.54  0.54 -0.54  0.54  1.62  3.78 -0.54 -0.54  0.54  0.54\n",
      "  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54  0.54  0.54\n",
      " -0.54 -0.54 -0.54  0.54 -0.54  0.54 -0.54  0.54  0.54  1.62 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54  3.78 -0.54  2.7  -0.54 -0.54  2.7  -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      "  0.54  1.62 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  2.7   0.54 -0.54 -0.54\n",
      " -0.54  0.54 -0.54  3.78 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  1.62\n",
      "  0.54  3.78 -0.54 -0.54 -0.54  2.7  -0.54  0.54  1.62  0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54  0.54 -0.54  0.54  1.62 -0.54 -0.54  0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  1.62 -0.54  2.7\n",
      " -0.54  1.62 -0.54 -0.54  0.54  1.62 -0.54 -0.54  3.78 -0.54  1.62 -0.54\n",
      " -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      "  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  3.78 -0.54 -0.54 -0.54  0.54\n",
      " -0.54 -0.54  3.78  1.62 -0.54 -0.54  0.54 -0.54  0.54 -0.54  0.54  0.54\n",
      "  0.54 -0.54 -0.54 -0.54  1.62  1.62 -0.54  0.54 -0.54  0.54  0.54  0.54\n",
      "  1.62 -0.54  0.54 -0.54  0.54 -0.54  1.62 -0.54 -0.54 -0.54 -0.54  0.54\n",
      "  0.54  0.54  0.54  0.54 -0.54 -0.54  0.54 -0.54 -0.54 -0.54  0.54  1.62\n",
      "  3.78  1.62  0.54  0.54  1.62 -0.54 -0.54 -0.54 -0.54  0.54 -0.54 -0.54\n",
      " -0.54 -0.54  0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  3.78 -0.54\n",
      "  1.62 -0.54 -0.54 -0.54 -0.54  0.54  1.62 -0.54 -0.54 -0.54 -0.54 -0.54\n",
      " -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54 -0.54  0.54  0.54\n",
      " -0.54 -0.54  2.7   0.54 -0.54 -0.54 -0.54 -0.54 -0.54  1.62 -0.54  0.54\n",
      " -0.54 -0.54 -0.54 -0.54  2.7  -0.54 -0.54  0.54 -0.54 -0.54 -0.54  1.62\n",
      "  1.62]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.   -1.    0.56 -1.   -0.22 -0.22 -1.   -1.   -1.   -1.   -0.22 -1.\n",
      "  0.56 -0.22 -1.   -0.22 -0.22 -1.    1.34  0.56 -1.   -1.    0.56  1.34\n",
      " -1.    0.56 -0.22  1.34 -1.    2.12  1.34 -1.   -1.   -1.   -1.   -1.\n",
      " -1.   -1.   -1.   -1.   -0.22  1.34 -1.   -1.   -0.22 -1.    1.34 -1.\n",
      " -0.22 -1.    0.56 -1.    1.34  0.56  1.34 -1.   -1.   -1.   -1.   -1.\n",
      "  0.56  2.12 -1.    1.34  1.34 -0.22  2.12 -0.22  0.56  0.56 -1.   -1.\n",
      "  1.34 -0.22  1.34  0.56 -1.    0.56 -1.   -0.22  0.56 -0.22 -1.    0.56\n",
      "  0.56  0.56 -0.22  0.56 -1.    2.12  0.56  0.56  0.56 -1.   -1.   -1.\n",
      " -1.   -1.   -0.22 -1.    2.12 -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
      "  2.12 -1.   -1.   -1.   -1.   -1.   -1.   -0.22 -1.   -1.    1.34 -0.22\n",
      " -0.22 -0.22 -0.22  1.34 -1.   -0.22 -1.   -1.    2.12 -0.22 -0.22  0.56\n",
      " -0.22 -0.22  0.56  1.34 -1.   -1.   -1.   -1.    1.34 -1.    2.12 -1.\n",
      " -0.22 -1.    0.56 -1.   -1.   -1.    0.56  0.56 -0.22 -1.    0.56 -0.22\n",
      " -0.22 -1.   -1.   -1.    1.34 -1.    2.12  2.12  2.12  0.56 -1.   -1.\n",
      " -1.    0.56  2.12 -0.22  1.34  2.12 -0.22  1.34  1.34  1.34  2.12 -1.\n",
      "  1.34 -1.   -1.   -1.    1.34 -1.    0.56 -1.   -1.    1.34  1.34  1.34\n",
      "  1.34 -1.    0.56 -0.22  0.56  0.56 -0.22  0.56 -0.22 -0.22  0.56  2.12\n",
      " -1.   -1.    2.12  1.34 -1.   -1.   -0.22  2.12  0.56 -0.22  2.12  0.56\n",
      "  0.56 -0.22 -1.    1.34 -0.22 -1.    1.34 -1.   -0.22  2.12 -1.    1.34\n",
      " -0.22  0.56  1.34  1.34  1.34 -1.   -0.22 -1.   -1.    2.12 -1.   -1.\n",
      "  0.56  0.56  2.12 -0.22 -0.22 -1.    0.56  1.34 -1.    0.56  2.12 -1.\n",
      " -0.22 -1.   -1.   -1.   -0.22 -0.22 -1.   -0.22 -0.22 -1.   -1.    2.12\n",
      " -1.    0.56 -1.    1.34  0.56 -1.   -0.22  1.34 -1.    0.56 -1.   -1.\n",
      " -1.   -0.22 -1.    2.12  0.56  1.34  1.34  0.56  2.12  0.56  1.34 -0.22\n",
      "  0.56 -1.   -1.    1.34  0.56  0.56 -1.   -1.   -0.22 -1.   -1.   -0.22\n",
      "  0.56 -1.   -1.   -1.   -1.    0.56  1.34  1.34 -0.22  0.56 -0.22  0.56\n",
      "  0.56 -1.   -0.22 -0.22 -1.    0.56 -1.    1.34 -1.   -1.    0.56 -0.22\n",
      "  1.34  1.34  1.34 -1.   -1.   -0.22 -1.   -0.22 -0.22 -1.    0.56 -1.\n",
      "  1.34 -0.22 -1.   -1.    0.56  0.56 -0.22 -1.   -0.22 -1.   -0.22 -1.\n",
      " -1.   -0.22 -1.   -1.   -0.22 -0.22 -0.22 -1.   -1.   -1.    1.34  2.12\n",
      " -1.   -1.    0.56 -1.   -0.22 -0.22 -1.   -0.22  1.34  0.56 -1.    1.34\n",
      " -1.   -0.22  0.56  0.56 -0.22 -0.22  2.12  2.12  0.56 -0.22  2.12  1.34\n",
      " -0.22 -1.   -1.   -0.22 -1.   -1.   -1.    0.56  0.56 -1.    0.56 -0.22\n",
      "  0.56 -0.22 -0.22  0.56 -0.22  0.56  0.56 -0.22  1.34  1.34 -1.   -1.\n",
      "  0.56  1.34  0.56 -0.22 -1.   -0.22  0.56  1.34 -0.22 -0.22  2.12  1.34\n",
      "  0.56 -1.   -0.22  0.56 -1.   -1.    0.56 -0.22 -1.    0.56 -1.   -0.22\n",
      " -0.22  2.12 -1.   -0.22 -0.22 -0.22  0.56 -1.    2.12  1.34 -0.22 -0.22\n",
      " -0.22  1.34 -1.    2.12 -1.   -1.   -0.22 -1.   -1.   -0.22 -0.22  0.56\n",
      " -1.    2.12 -1.   -1.   -1.    0.56 -0.22 -0.22  1.34  0.56  0.56 -1.\n",
      " -0.22 -1.   -1.    0.56 -1.    0.56  2.12 -1.   -1.    1.34 -1.   -1.\n",
      " -1.   -1.    0.56 -0.22 -0.22 -0.22  1.34 -1.   -1.    1.34 -1.   -1.\n",
      " -1.    2.12  0.56 -1.   -0.22  1.34 -1.   -0.22  2.12  2.12  2.12 -1.\n",
      " -1.   -0.22  1.34 -1.   -0.22 -1.   -1.   -1.   -1.    0.56  1.34  1.34\n",
      "  0.56  1.34  0.56 -1.    0.56 -1.   -0.22  2.12 -1.   -0.22 -0.22 -0.22\n",
      " -0.22 -0.22  2.12  0.56  1.34 -1.    2.12  0.56  0.56 -0.22 -0.22 -0.22\n",
      " -0.22 -1.    1.34  0.56  0.56  1.34 -1.   -1.   -0.22  0.56 -0.22 -0.22\n",
      "  1.34 -1.   -0.22 -1.    1.34 -1.    2.12 -1.    0.56  1.34 -1.    0.56\n",
      "  0.56  1.34  0.56 -0.22 -1.    0.56  1.34  0.56 -1.   -0.22  1.34  1.34\n",
      "  2.12  0.56  0.56  0.56  0.56 -1.   -1.   -1.   -1.   -0.22  0.56 -0.22\n",
      " -1.   -1.   -0.22 -1.   -1.   -0.22 -0.22 -1.   -0.22 -1.    2.12 -1.\n",
      "  1.34 -1.    1.34 -1.   -0.22  0.56 -1.   -0.22 -0.22 -1.    1.34 -1.\n",
      "  0.56 -1.   -0.22 -0.22 -0.22 -1.    1.34  0.56 -1.   -0.22  1.34 -0.22\n",
      " -0.22 -0.22 -0.22  0.56 -1.    0.56 -1.   -0.22 -1.    1.34 -0.22  0.56\n",
      "  1.34  0.56 -1.    0.56  0.56  0.56 -1.   -0.22 -0.22 -1.   -1.    1.34\n",
      "  1.34]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.07 -0.36  0.5  -0.79 -0.79  0.5  -0.79 -0.36 -0.79 -0.79 -0.36 -0.79\n",
      " -0.79 -0.79 -0.79  0.5   1.37 -0.36 -0.36  0.5  -0.79 -0.79 -0.79 -0.36\n",
      " -0.36  0.5   0.94 -0.79 -0.36  0.07 -0.79 -0.36 -0.79 -0.79  0.07  0.07\n",
      " -0.79  0.07 -0.36  0.94  2.66  0.94 -0.79 -0.79  2.23  0.07  0.5  -0.36\n",
      "  0.07 -0.36 -0.79 -0.79  0.07 -0.79 -0.79 -0.36 -0.79  0.94 -0.79 -0.36\n",
      " -0.79 -0.79 -0.79 -0.79 -0.79 -0.36 -0.79 -0.79 -0.79  0.5  -0.36 -0.79\n",
      " -0.36 -0.79  0.07 -0.79 -0.79 -0.57 -0.79  2.23 -0.79 -0.36  0.07 -0.36\n",
      " -0.36  0.07  0.5   0.07  0.5   0.5  -0.36  0.5  -0.79 -0.36 -0.36  0.07\n",
      "  0.5  -0.79  0.07 -0.36  1.8  -0.79 -0.36  2.66 -0.79  1.37  0.07  0.07\n",
      "  0.94 -0.36  0.07  0.07  1.8   1.37  0.07  0.5   0.07  0.5   2.23 -0.36\n",
      " -0.79  0.5  -0.36  0.07 -0.79 -0.36 -0.79 -0.79  0.94  0.07 -0.79  1.37\n",
      "  0.94  0.5  -0.36  1.8   0.07  1.8  -0.36  0.07 -0.36 -0.79 -0.36  1.37\n",
      "  0.94  0.07 -0.79  0.5  -0.36  0.5   4.39 -0.79  0.07 -0.36  0.07  3.96\n",
      " -0.79 -0.79 -0.36  0.5   0.5   2.66 -0.79 -0.79  0.5  -0.36  0.5  -0.79\n",
      " -0.79  2.23  0.5   0.07 -0.79 -0.79  0.94  0.07 -0.36 -0.36 -0.79  1.37\n",
      " -0.79 -0.79 -0.79 -0.79  0.07 -0.79 -0.79 -0.79 -0.79 -0.79 -0.79  0.07\n",
      "  0.94  0.07  0.07 -0.79 -0.79  6.11  0.94  0.5  -0.79 -0.79  1.37  0.5\n",
      "  0.5  -0.79  2.66 -0.79 -0.79  0.94  0.07  2.66  5.68 -0.79 -0.36  0.07\n",
      " -0.79  3.74  0.5   2.23 -0.36  0.07 -0.36 -0.79  0.07  2.45 -0.79  1.37\n",
      "  0.5  -0.36  2.66  1.37  0.07 -0.79 -0.79  1.8  -0.79  1.15  0.07  0.07\n",
      " -0.36 -0.79 -0.36 -0.79 -0.79 -0.79 -0.79 -0.79 -0.36 -0.79 -0.79  0.07\n",
      " -0.79  3.09  2.66  2.23  4.82  0.5   0.07  1.37  0.07 -0.36  2.23  2.66\n",
      "  0.94  0.07  0.07  0.94 -0.79  0.72  0.07 -0.36 -0.36  1.37  1.37  1.37\n",
      "  0.72  0.07 -0.36  1.8   0.94 -0.79  0.94  0.07  0.07 -0.36 -0.79 -0.79\n",
      " -0.36 -0.36 -0.79  0.07 -0.36  0.07  0.07  0.07 -0.36 -0.79 -0.36  0.94\n",
      " -0.36 -0.36 -0.36 -0.36 -0.79  0.94  0.5  -0.79  0.94  0.07  1.8   2.66\n",
      "  1.37 -0.36 -0.36  0.94  0.5   0.5   2.45  0.5  -0.36  0.94 -0.79 -0.79\n",
      "  1.37  3.96  3.09 -0.36 -0.79  1.8   1.37 -0.79 -0.79 -0.79 -0.79 -0.79\n",
      " -0.36 -0.79 -0.79 -0.79 -0.79 -0.36 -0.79 -0.79 -0.79 -0.79 -0.79 -0.79\n",
      " -0.79 -0.79 -0.79 -0.79 -0.79 -0.36 -0.36 -0.36 -0.36 -0.36  0.94 -0.36\n",
      "  0.94  1.15 -0.79  0.07 -0.36  2.23 -0.79 -0.36 -0.36  1.8  -0.36 -0.36\n",
      "  0.07 -0.79 -0.79 -0.79 -0.79 -0.79 -0.79 -0.36 -0.79  0.07 -0.36 -0.79\n",
      " -0.79 -0.36 -0.79  0.5   0.94 -0.79  0.5   0.94 -0.79  0.07 -0.79 -0.79\n",
      "  0.5   3.09 -0.79  0.07  0.07 -0.79 -0.79  1.37  0.07  2.66  0.5  -0.79\n",
      "  1.58  1.15 -0.79 -0.36 -0.79  3.74  0.94  0.29  1.37  0.29  2.01  1.37\n",
      "  1.37  0.29  0.07  1.58 -0.79  0.5   0.07 -0.79 -0.79  0.29 -0.79 -0.79\n",
      " -0.79 -0.79  0.07 -0.79  0.07 -0.79 -0.36 -0.79 -0.79  0.07 -0.79  0.72\n",
      "  0.07  0.94 -0.79  0.94  0.5  -0.14 -0.36 -0.79  0.07 -0.79 -0.14 -0.79\n",
      " -0.79  0.07 -0.79 -0.36 -0.57 -0.79 -0.79 -0.36  0.5   1.15 -0.79 -0.57\n",
      " -0.57 -0.79  0.07 -0.36 -0.79  0.07 -0.36 -0.79 -0.36  1.58 -0.79  0.07\n",
      " -0.79 -0.36 -0.79 -0.57  0.5  -0.79 -0.57  0.5   0.5  -0.79  0.07  1.8\n",
      " -0.36  0.29 -0.79 -0.79 -0.36 -0.79 -0.79 -0.79  1.8   0.94 -0.36 -0.79\n",
      " -0.79 -0.79  0.94 -0.79 -0.79 -0.79 -0.79 -0.57  1.15 -0.79 -0.36  0.94\n",
      " -0.79 -0.79  0.94 -0.79  0.5  -0.79  0.07 -0.36 -0.36 -0.79 -0.57 -0.79\n",
      "  0.29 -0.57  0.94 -0.79 -0.36 -0.79 -0.79  1.58  0.94  1.58 -0.36  0.94\n",
      "  0.29 -0.79 -0.57 -0.36 -0.36  0.94 -0.79 -0.79 -0.36 -0.79 -0.79 -0.79\n",
      " -0.36  0.07 -0.14  0.29  0.07 -0.36 -0.79 -0.79 -0.79  0.5  -0.79 -0.79\n",
      " -0.36 -0.79 -0.36 -0.79  0.07  1.15  0.5   0.07  0.94  1.15  1.8   0.94\n",
      "  0.94  0.94  0.07 -0.79  0.07 -0.36 -0.79 -0.79 -0.79  0.29 -0.79 -0.14\n",
      " -0.79 -0.79 -0.14 -0.36 -0.79 -0.79 -0.79 -0.14 -0.79 -0.79  0.94  0.29\n",
      "  0.07  0.07 -0.79 -0.79 -0.36 -0.79 -0.79  0.29 -0.36 -0.79 -0.79  0.29\n",
      "  0.07 -0.79 -0.36 -0.79 -0.36 -0.79 -0.79  0.5   0.07 -0.79 -0.79 -0.79\n",
      " -0.79 -0.79 -0.79 -0.14  0.94  0.07 -0.79  0.07  0.07 -0.57 -0.57  1.37\n",
      "  0.07 -0.79  0.07 -0.79 -0.79 -0.79 -0.79  0.07  0.07  0.07  0.5   0.5\n",
      "  0.07]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-4.16 -0.87  0.22  0.95 -0.15  0.22  0.58 -0.51  1.31  0.22  0.95 -0.51\n",
      "  0.22  0.22  0.95  2.04  0.58  0.58 -1.24  0.22  0.22 -0.15  0.22 -0.51\n",
      " -0.51 -0.51 -0.15 -0.15  0.22  0.22 -0.51  1.31  0.58  0.58  0.22 -0.15\n",
      "  0.95  0.58 -0.15  0.95 -0.15 -0.51  0.95 -0.87 -0.51 -0.51  0.58  2.04\n",
      " -0.15  0.58  0.95  1.68 -0.51  0.58  0.58  0.22  1.31  1.31  0.95  1.68\n",
      "  2.04 -0.51  0.58  0.95  0.58  1.68 -0.15 -0.51 -0.15  1.31  0.58 -0.15\n",
      "  0.58  0.58 -0.15 -0.15  0.22  0.58 -0.87  0.22 -0.15 -0.51  0.22  0.58\n",
      "  0.58  0.22  0.58  1.31  0.22 -0.87 -0.87  0.95  0.22  0.58 -0.15  0.58\n",
      " -0.87  0.58  0.22  0.22 -0.87  1.68  0.22 -0.15  1.68 -0.51 -0.51  0.58\n",
      "  0.22  1.31  0.58 -0.15 -1.24  2.41 -0.51  1.68  1.68  0.95  0.22  0.95\n",
      "  0.95  0.95  0.95  0.22  0.22 -0.87 -0.51 -0.87  0.95  0.58 -0.51 -0.51\n",
      " -0.15 -0.15  0.58  1.31 -0.87  0.58  0.22  0.58 -0.51  0.95 -0.15  0.22\n",
      "  0.22 -0.87 -0.51  0.58 -1.24 -0.87 -0.87  1.31  0.22  0.58 -0.51 -0.87\n",
      "  0.22 -0.15 -0.15  0.58 -0.15 -0.87  0.22 -0.15 -0.87  0.58 -0.15  0.58\n",
      " -1.6  -0.87 -0.15  0.22 -0.51 -0.87 -1.24 -1.6  -1.24 -0.87 -1.24 -1.24\n",
      " -0.87  2.04  0.22  1.68 -0.87  1.68 -0.15  0.95  0.95 -0.51  0.58 -0.15\n",
      " -0.51 -0.15 -0.15 -0.15  2.04  0.95  0.95 -0.15 -0.51  0.58  0.22 -0.87\n",
      "  0.22  0.22 -0.51 -0.15  0.58  0.95  0.58 -0.51  0.95 -0.15  0.95 -0.15\n",
      "  0.95  0.58  0.58 -1.24 -0.15  0.95  0.22 -0.15  0.22  0.58  0.58  0.22\n",
      "  0.22  0.95 -0.15 -0.51  0.22  0.22 -0.15  0.22  0.58 -0.51  1.31 -1.24\n",
      "  2.04 -0.51 -0.87  1.31  0.95  0.22  0.58  0.58 -0.87  0.22  1.68 -0.87\n",
      "  0.95 -0.51 -0.87 -1.24 -1.6  -0.51  0.95 -0.15  0.22  0.58 -0.87 -1.24\n",
      " -0.15  1.31  1.31  1.31  0.22  0.95  0.95 -0.15  0.58 -0.15 -0.51  0.22\n",
      "  1.31 -0.15 -0.87 -1.6  -0.51 -0.15 -1.6  -0.87 -1.6   0.95  0.22 -0.87\n",
      "  0.22  0.95 -0.51 -0.15 -0.15 -0.15 -0.15 -0.15  1.68 -0.87 -0.51 -0.51\n",
      "  1.31 -0.51 -0.51  1.31 -0.15 -1.24  1.31 -0.15 -0.51 -0.51 -1.24 -0.15\n",
      "  1.68  0.58  1.68  0.95  1.31  0.22  0.22 -0.15  0.95  0.58 -0.87 -0.51\n",
      " -0.51 -0.15  0.58  2.04  0.22  0.22  0.22  0.22  2.41  0.58  0.95  2.04\n",
      "  1.31  2.04  2.41  0.95  0.95  0.95  0.58  1.68  2.41  0.95  0.22 -1.6\n",
      "  1.68  2.41 -0.87  0.95 -0.51 -0.51  0.22 -0.15  1.31  0.95  0.22  1.31\n",
      " -0.15 -0.15  0.22 -0.15  0.95  1.31  0.22 -0.15 -1.24 -0.51 -1.24 -0.51\n",
      "  0.95 -0.15  2.04  0.95  0.95  0.58  2.04 -0.15  0.58  1.31 -0.15 -0.15\n",
      " -0.15 -0.51  1.31  1.31 -0.51 -0.51  0.95 -0.15  0.22  0.95  0.95  0.22\n",
      "  1.68 -0.51 -0.15  0.95  1.31  0.95 -0.15  0.22  0.22 -0.51 -0.51 -0.87\n",
      " -0.87  0.58  0.58  0.22  0.95 -0.87 -0.15 -0.87  1.68  0.95  0.58 -1.6\n",
      "  0.95 -1.24 -0.51 -0.51  0.22 -0.51 -0.51  1.68 -1.97 -0.51 -0.51 -0.51\n",
      " -1.97  0.58 -0.51 -0.51 -1.97  0.22 -0.51  0.95 -1.6  -1.24  0.22 -1.6\n",
      " -0.87 -1.6  -0.15 -1.24  1.68 -0.15  1.31 -0.51 -0.87 -0.51 -0.87 -0.87\n",
      "  0.58 -0.51  0.58 -0.51 -0.15  0.58  0.58  0.95 -0.15 -0.87 -0.51  0.95\n",
      "  0.58  0.58  0.58  0.22  0.95 -0.51 -0.51  0.22 -0.51  0.22 -1.6  -0.87\n",
      " -0.87 -0.87 -0.87 -0.87 -1.6  -0.15 -0.87 -1.6  -1.6  -1.24 -0.87 -1.6\n",
      " -1.24 -0.87 -1.24  0.95  0.95 -0.51  0.95  2.04 -1.97  0.95 -0.51  0.95\n",
      "  0.22 -0.15 -0.15 -0.15 -0.51  1.68  1.31 -0.15 -1.6  -1.24 -1.6  -0.87\n",
      "  0.95  0.95 -1.24 -1.24 -1.97 -1.24 -1.24 -2.33 -1.6   1.31  0.58 -0.51\n",
      " -0.87 -0.15 -1.6  -0.51 -0.87  0.95 -0.15 -0.87  0.58  0.22 -0.15 -0.87\n",
      " -0.87 -0.15 -0.51 -0.87  0.22 -1.24  0.58 -0.51 -0.15  2.04  0.22  0.58\n",
      " -0.87 -0.15 -0.51  0.58 -0.87 -1.6  -1.24  0.22 -0.51 -0.51 -0.15 -1.6\n",
      " -0.15 -0.51 -0.87 -2.7  -1.97 -2.7  -1.6  -1.24 -2.33 -1.6  -1.24 -0.51\n",
      " -0.87 -0.51 -1.24 -0.87 -1.24 -1.97 -1.97 -1.24 -0.51 -1.24 -1.24 -1.6\n",
      " -1.24 -1.6  -1.6   0.22 -0.15  0.22  2.41  2.04  2.04 -0.87 -0.51  0.22\n",
      "  0.22 -1.24 -0.15 -2.33 -0.87 -2.33  2.41 -0.15 -1.24 -0.15 -1.24  0.95\n",
      " -0.51 -0.51  0.22  1.31  0.95  2.77  1.68  0.58  1.31  0.58 -1.24  1.31\n",
      " -1.24 -0.51 -1.6  -0.87 -0.51 -1.6   1.31 -0.51 -1.6   0.58  1.68 -1.24\n",
      "  2.04 -1.6   0.95 -2.33 -1.6   0.95 -1.97 -1.6  -0.51  1.31 -0.15 -0.51\n",
      " -0.51]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
      "/var/folders/5s/xsxh_mvj2tz0s4q_6f7z01mm0000gn/T/ipykernel_68084/2480942635.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.2  -0.2   0.49  0.83  0.49  0.15  0.15  0.49  1.52  0.15  0.83  0.15\n",
      "  0.49  0.15  0.83  1.87  0.49  0.83 -1.23  0.15  0.49  0.15  0.49 -0.54\n",
      " -0.2  -0.2   0.15 -0.2   0.15 -0.2  -0.2   1.18  0.83  0.15  0.15 -0.2\n",
      "  0.83  0.49  0.15  0.49 -0.2  -0.2   1.18 -0.54 -0.2  -0.2   0.15  1.87\n",
      "  0.15  0.15  0.49  0.83 -0.88  0.15  0.15  0.49  0.83  1.18  0.49  1.18\n",
      "  1.52 -0.54  0.49  0.49  0.15  1.18  0.15 -0.88 -0.54  1.18 -0.2  -0.88\n",
      " -0.2   0.49 -0.2  -0.2  -0.2   0.49 -0.88 -0.2  -0.2  -0.88 -0.2   0.15\n",
      "  0.15 -0.2  -0.2   1.18 -0.54 -0.88 -0.2   0.49  0.49  0.83  0.15  0.49\n",
      " -0.2   0.15  0.49  0.49 -0.88  1.52  0.49 -0.54  1.52 -0.54 -0.54  0.83\n",
      " -0.54  1.18  0.83 -0.2  -0.54  1.87 -0.88  0.83  1.18  0.83 -0.2   0.49\n",
      "  0.83  0.49  0.15  0.49 -0.2  -0.88 -0.2  -0.54  0.83  0.15 -0.2  -0.88\n",
      " -0.54  0.15  0.15  0.49 -0.88 -0.2  -0.2   0.83 -0.88  0.49 -0.2  -0.2\n",
      " -0.2  -0.88 -0.54  0.83 -0.88 -0.2  -1.23  0.49 -0.2   0.49 -0.2  -1.57\n",
      "  0.15 -0.54 -0.2   0.15 -0.2  -0.88 -0.2  -0.88 -0.54  0.49 -0.88  0.49\n",
      " -1.23 -1.23 -0.54 -0.54 -0.54 -1.23 -1.23 -1.91 -0.88 -1.23 -1.23 -1.57\n",
      " -0.54  1.87  0.15  1.52 -0.88  1.87 -0.88  0.49  0.83 -0.2   0.15 -0.88\n",
      " -0.88 -0.2  -0.54  0.15  2.21  0.49  1.18  0.49 -0.88  0.83  0.49 -0.88\n",
      "  0.15 -0.2  -0.2  -0.54  0.15  1.18  0.49 -0.2   1.18 -0.54  0.83  0.15\n",
      "  1.18  0.49  0.49 -0.88 -0.2   1.18  0.49  0.15  0.15  0.15  0.49 -0.2\n",
      " -0.2   1.52 -0.88 -0.54  0.49  0.15 -0.54  0.15  0.49 -0.88  1.52 -1.23\n",
      "  2.21 -0.2  -0.54  1.18  0.15  0.49  0.49  0.49 -0.88  0.15  1.52 -0.54\n",
      "  0.49 -0.54 -1.23 -1.57 -1.23 -1.23  0.83 -0.54 -0.2   0.49 -0.88 -1.57\n",
      " -0.88  0.83  0.83  1.18  0.15  0.83  1.18 -0.2   0.83 -0.2  -0.88 -0.54\n",
      "  1.52 -0.54  0.15 -1.23 -0.88  0.15 -1.23 -0.2  -1.23  0.49  0.15 -0.54\n",
      "  0.15  1.18 -0.54 -0.54  0.15  0.15 -0.2   0.49  1.52 -0.54 -0.2   0.15\n",
      "  1.18 -0.54 -0.2   0.15  0.15 -1.23  1.52 -0.2  -0.2   0.15 -0.88  0.15\n",
      "  1.52  0.49  1.87  0.83  1.52  0.15 -0.88  0.15  0.15  1.18 -1.23 -0.54\n",
      " -0.54 -0.2   0.49  1.87  0.49  0.15  0.49  0.15  2.21  0.49  1.18  1.87\n",
      "  1.52  2.21  2.55  1.18  1.18  0.49  0.83  1.87  2.21  1.18  0.49 -1.57\n",
      "  1.52  2.21 -0.54  1.18 -0.54 -0.54  0.15  0.15  1.52  1.18  0.15  1.18\n",
      "  0.15 -0.54 -0.2   0.15  1.18  0.83  0.15  0.15 -0.88 -0.54 -1.23 -0.54\n",
      "  0.49 -0.2   2.21  0.83  0.83  0.83  2.21 -0.54  0.15  0.83  0.15 -0.2\n",
      " -0.54 -0.88  1.18  0.83 -0.2  -0.88  0.49  0.49  0.49  0.83  0.83  0.15\n",
      "  1.52  0.15  0.15  1.52  0.83  0.49  0.15  1.18  1.52 -0.2   0.49  0.15\n",
      " -0.2   0.83  1.87  1.18  1.87 -0.54  1.18 -0.54  2.21  1.87  0.83 -0.2\n",
      "  1.87 -0.54  0.49 -0.2   0.15 -0.54 -0.54  1.87 -1.23 -0.2  -0.2  -0.54\n",
      " -1.91  0.83 -0.88 -0.54 -1.57 -0.2  -0.2   1.52 -3.97 -0.88  0.49 -0.88\n",
      " -0.54 -0.88 -0.2  -0.54  1.52  0.49  1.18 -0.2  -0.54 -1.23 -1.23 -0.54\n",
      "  0.83 -0.2  -0.54 -0.88 -0.54  0.15  0.83 -0.2  -0.2  -1.23 -0.88  0.49\n",
      "  0.15  0.15  0.15 -0.2   0.83 -1.23 -0.2   0.49 -1.23 -0.54 -1.57 -0.88\n",
      " -0.88 -0.54 -0.54 -0.54 -1.57 -0.54 -0.54 -0.88 -0.88 -0.88 -0.2  -1.23\n",
      " -0.54 -0.88 -0.88  0.49  0.83 -0.2   0.49  1.52 -1.57  0.15 -1.23  0.83\n",
      "  0.49 -0.2  -0.88 -0.2  -0.88  1.87  1.87 -0.88 -1.57 -1.91 -1.91 -1.23\n",
      "  1.52  0.83 -2.26 -1.57 -1.23 -0.54 -1.23 -1.91 -0.88  0.83  0.83 -0.88\n",
      " -0.88 -0.2  -0.54 -0.54 -1.57  1.18  0.15 -0.2   0.83  0.49 -0.2  -0.54\n",
      " -0.2  -0.88 -0.2  -0.88 -0.2  -0.88  1.18 -0.2  -0.54  2.21  0.49  0.83\n",
      " -0.88 -0.2  -0.2   0.83 -0.54 -0.88  0.49  0.15 -0.88 -0.88  0.49 -3.97\n",
      " -0.88 -0.54 -1.57 -3.97 -0.2  -1.23 -1.23 -0.88 -2.26 -0.88 -0.2  -0.2\n",
      " -0.54 -0.88 -0.88 -0.54 -0.88 -1.23 -1.23 -1.91 -0.2  -1.57 -1.23 -1.91\n",
      " -0.54 -0.54 -1.23  0.49  0.15  0.49  2.21  1.87  2.21 -3.97 -0.2   0.49\n",
      "  0.49 -1.23  0.15 -3.97 -0.54 -3.97  2.21 -0.2  -0.54 -0.2  -3.97  0.83\n",
      " -0.2  -0.54  0.15  0.83  0.49  1.87  1.18  0.15  0.83  0.49 -0.54  1.52\n",
      " -1.23 -0.54 -2.26 -0.54 -0.2  -1.23  1.87 -0.2  -1.23  0.83  1.52 -0.88\n",
      "  2.21 -1.57  1.18 -1.23 -1.57  1.87 -0.88 -0.88 -0.2   1.18  0.15 -0.54\n",
      " -0.2 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Copy the DataFrame to avoid working on the original\n",
    "df_encoded = por.copy()\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['Medu', 'failures', 'Dalc', 'Walc', 'absences', 'G1', 'G2']\n",
    "categorical_features = por.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Label encode categorical features\n",
    "for col in categorical_features:\n",
    "    df_encoded[col] = LabelEncoder().fit_transform(df_encoded[col])\n",
    "\n",
    "# Define the target column and features\n",
    "target_column = \"G3\"\n",
    "X = df_encoded[['Medu', 'failures', 'Dalc', 'Walc', 'absences', 'G1', 'G2', 'sex', \n",
    "                'Mjob', 'schoolsup', 'famsup', 'activities', 'nursery', 'higher']].copy()\n",
    "y = df_encoded[target_column].copy()\n",
    "\n",
    "# Scale the numerical features\n",
    "sc_X = StandardScaler()\n",
    "X.loc[:, numerical_features] = sc_X.fit_transform(X[numerical_features])  # Use `.loc` for explicit indexing\n",
    "\n",
    "# Scale the target dependent variable\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize Support Vector Regression (SVR) with linear kernel\n",
    "regressor = SVR(kernel='linear')\n",
    "\n",
    "# Fit the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Rescale predictions and actual values back to original scale\n",
    "y_pred_original = sc_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = sc_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Set NumPy print options to suppress scientific notation\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "# Display predictions vs actual values\n",
    "comparison = np.column_stack((y_pred_original, y_test_original))\n",
    "print(\"Predictions vs Actual (in fixed-point notation):\")\n",
    "print(comparison)\n",
    "\n",
    "# Evaluate the model performance\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold cross validation and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation R² scores for each fold: [0.81 0.76 0.85 0.89 0.8 ]\n",
      "Average Cross-Validation R² score: 0.821046267780768\n",
      "Best Hyperparameters from RandomizedSearchCV: {'kernel': 'linear', 'epsilon': np.float64(0.07), 'C': np.float64(10.0)}\n",
      "Best R² score from RandomizedSearchCV: 0.827946787622075\n",
      "Predictions vs Actual:\n",
      "[[ 7.69  8.  ]\n",
      " [14.5  15.  ]\n",
      " [16.24 16.  ]\n",
      " [10.43 10.  ]\n",
      " [ 9.47 10.  ]\n",
      " [12.58 12.  ]\n",
      " [13.11 13.  ]\n",
      " [18.25 17.  ]\n",
      " [12.06 12.  ]\n",
      " [11.31 12.  ]\n",
      " [10.61 11.  ]\n",
      " [10.55 10.  ]\n",
      " [13.12 13.  ]\n",
      " [ 8.67  8.  ]\n",
      " [18.04 18.  ]\n",
      " [12.31 12.  ]\n",
      " [12.78 13.  ]\n",
      " [12.6  13.  ]\n",
      " [10.6  10.  ]\n",
      " [10.23 10.  ]\n",
      " [12.07 12.  ]\n",
      " [10.42 10.  ]\n",
      " [17.3  17.  ]\n",
      " [13.3  15.  ]\n",
      " [12.59 14.  ]\n",
      " [ 1.75  0.  ]\n",
      " [12.76 12.  ]\n",
      " [13.45 14.  ]\n",
      " [11.33 12.  ]\n",
      " [12.8   9.  ]\n",
      " [13.56 13.  ]\n",
      " [16.24 16.  ]\n",
      " [13.19 13.  ]\n",
      " [16.02 16.  ]\n",
      " [12.38 12.  ]\n",
      " [ 9.17 10.  ]\n",
      " [ 9.64 10.  ]\n",
      " [11.33 11.  ]\n",
      " [12.47 13.  ]\n",
      " [11.5  10.  ]\n",
      " [15.32 15.  ]\n",
      " [17.24 18.  ]\n",
      " [11.43 11.  ]\n",
      " [13.63 13.  ]\n",
      " [12.5  13.  ]\n",
      " [ 9.66 10.  ]\n",
      " [12.71 14.  ]\n",
      " [ 9.09  9.  ]\n",
      " [11.44 11.  ]\n",
      " [ 9.58 10.  ]\n",
      " [ 6.24  8.  ]\n",
      " [14.69 17.  ]\n",
      " [ 9.3   9.  ]\n",
      " [12.21 13.  ]\n",
      " [ 7.28  8.  ]\n",
      " [11.2  11.  ]\n",
      " [11.59 12.  ]\n",
      " [10.94 12.  ]\n",
      " [14.6  15.  ]\n",
      " [14.56 15.  ]\n",
      " [13.53 13.  ]\n",
      " [ 7.58  7.  ]\n",
      " [11.73 12.  ]\n",
      " [ 9.25 10.  ]\n",
      " [13.46 12.  ]\n",
      " [12.31 12.  ]\n",
      " [12.   11.  ]\n",
      " [13.25 13.  ]\n",
      " [14.37 14.  ]\n",
      " [ 7.01  8.  ]\n",
      " [ 8.45  9.  ]\n",
      " [11.31 11.  ]\n",
      " [13.74 13.  ]\n",
      " [10.95 11.  ]\n",
      " [14.12 14.  ]\n",
      " [13.51 13.  ]\n",
      " [13.8  14.  ]\n",
      " [11.61 13.  ]\n",
      " [13.58 13.  ]\n",
      " [12.25 13.  ]\n",
      " [14.52 14.  ]\n",
      " [ 9.39 11.  ]\n",
      " [10.4  10.  ]\n",
      " [13.14 14.  ]\n",
      " [18.54 17.  ]\n",
      " [11.41 13.  ]\n",
      " [10.51 10.  ]\n",
      " [13.8  12.  ]\n",
      " [13.41 13.  ]\n",
      " [13.22 10.  ]\n",
      " [11.33 12.  ]\n",
      " [15.8  16.  ]\n",
      " [17.95 17.  ]\n",
      " [11.68 11.  ]\n",
      " [ 7.76  6.  ]\n",
      " [10.31 11.  ]\n",
      " [13.49 14.  ]\n",
      " [11.3  11.  ]\n",
      " [13.58 13.  ]\n",
      " [14.48 15.  ]\n",
      " [12.35 14.  ]\n",
      " [ 9.51 10.  ]\n",
      " [ 6.17  8.  ]\n",
      " [10.96 11.  ]\n",
      " [10.29 10.  ]\n",
      " [11.36 12.  ]\n",
      " [16.4  17.  ]\n",
      " [10.55 11.  ]\n",
      " [ 9.84  9.  ]\n",
      " [15.26 15.  ]\n",
      " [11.87 12.  ]\n",
      " [13.32 13.  ]\n",
      " [14.54 14.  ]\n",
      " [13.29 14.  ]\n",
      " [11.73 12.  ]\n",
      " [10.12 11.  ]\n",
      " [15.69 15.  ]\n",
      " [16.24 16.  ]\n",
      " [10.61 10.  ]\n",
      " [11.31 12.  ]\n",
      " [ 7.86  8.  ]\n",
      " [10.63 11.  ]\n",
      " [10.64 11.  ]\n",
      " [11.   11.  ]\n",
      " [13.34 14.  ]\n",
      " [15.23 15.  ]\n",
      " [14.94 14.  ]\n",
      " [16.91 15.  ]\n",
      " [12.51 10.  ]\n",
      " [10.04 11.  ]]\n",
      "R² score on the test set: 0.8802525595059679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define the target column and features\n",
    "target_column = \"G3\"\n",
    "X = por.drop(columns=[target_column])\n",
    "y = por[target_column]\n",
    "\n",
    "# Apply the scaler only to numerical columns\n",
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "sc_X = StandardScaler()\n",
    "X[numerical_cols] = sc_X.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Encode categorical columns (if any)\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_features:\n",
    "    X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "# Scale the target column\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y.values.reshape(-1, 1)).flatten()  # Flatten back to 1D array\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize Support Vector Regression (SVR) with a linear kernel\n",
    "regressor = SVR(kernel='linear')\n",
    "\n",
    "# 1. K-Fold Cross-Validation (5-fold)\n",
    "cv_scores = cross_val_score(regressor, X, y, cv=5, scoring='r2')  # Using R² score for regression\n",
    "print(f\"Cross-Validation R² scores for each fold: {cv_scores}\")\n",
    "print(f\"Average Cross-Validation R² score: {cv_scores.mean()}\")\n",
    "\n",
    "# 2. Hyperparameter Tuning using RandomizedSearchCV\n",
    "# Define the parameter distribution for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'C': np.logspace(-3, 3, 7),  # Regularization parameter\n",
    "    'epsilon': np.linspace(0.01, 0.1, 10),  # Margin of tolerance\n",
    "    'kernel': ['linear', 'rbf'],  # Kernel type: linear or RBF\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=regressor, param_distributions=param_dist, n_iter=10, cv=5, scoring='r2', random_state=0)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and best R² score\n",
    "print(f\"Best Hyperparameters from RandomizedSearchCV: {random_search.best_params_}\")\n",
    "print(f\"Best R² score from RandomizedSearchCV: {random_search.best_score_}\")\n",
    "\n",
    "# 3. Fit the model with the best hyperparameters found by RandomizedSearchCV\n",
    "best_regressor = random_search.best_estimator_\n",
    "best_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the test data\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "# Rescale predictions and actual values back to the original scale\n",
    "y_pred_original = sc_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "y_test_original = sc_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Display predictions vs actual values\n",
    "comparison = np.column_stack((y_pred_original, y_test_original))\n",
    "print(\"Predictions vs Actual:\")\n",
    "print(comparison)\n",
    "\n",
    "# Evaluating the model performance on the test set\n",
    "r2_test_score = r2_score(y_test, y_pred)\n",
    "print(f\"R² score on the test set: {r2_test_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR performance (r2): \n",
    "\n",
    "with all features 0.7878634076745303, 0.8775693192610926 por\n",
    "with selected features 0.7736 math, 0.8737 por\n",
    "with hyperparameter tuning  0.782517153964617,  0.8802525595059679 for por"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
